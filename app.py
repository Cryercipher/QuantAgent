import asyncio
import os
from datetime import datetime
from importlib import import_module
from llama_index.core.agent import ReActAgent
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core import Settings, set_global_handler

from core.llm_factory import ModelFactory
from tools.market_data import MarketDataManager
from tools.quant_analysis import QuantAnalyzer
from tools.knowledge_base import FinancialKnowledgeBase
from prompts.system_prompts import AGENT_SYSTEM_PROMPT, AGENT_CONTEXT_INJECTION
from utils.logger import get_logger
from config.settings import PHOENIX_ENABLED, PHOENIX_HOST, PHOENIX_PORT

logger = get_logger("MainApp")


def init_phoenix_monitor():
    if not PHOENIX_ENABLED:
        logger.info("Phoenix ç›‘æŽ§å·²å…³é—­ã€‚è®¾ç½® PHOENIX_ENABLED=true ä»¥å¯ç”¨ã€‚")
        return None
    try:
        phoenix_module = import_module("phoenix")
    except Exception as exc:
        logger.warning(f"æœªå®‰è£… phoenixï¼Œè·³è¿‡ç›‘æŽ§ï¼š{exc}")
        return None

    os.environ.setdefault("PHOENIX_HOST", PHOENIX_HOST)
    os.environ.setdefault("PHOENIX_PORT", str(PHOENIX_PORT))
    os.environ.setdefault("PHOENIX_GRPC_PORT", str(PHOENIX_GRPC_PORT))

    try:
        session = phoenix_module.launch_app()
        session_name = getattr(session, "session_name", "N/A")
        set_global_handler("arize_phoenix")
        logger.info(
            f"Phoenix ç›‘æŽ§å·²å¯åŠ¨ï¼šhttp://{PHOENIX_HOST}:{PHOENIX_PORT} (session={session_name})"
        )
        return session
    except RuntimeError as exc:
        logger.error(
            "Phoenix å¯åŠ¨å¤±è´¥ï¼ˆå¯èƒ½ä¸ºç«¯å£å†²çªï¼šHTTP=%s, gRPC=%sï¼‰ï¼š%s",
            PHOENIX_PORT,
            PHOENIX_GRPC_PORT,
            exc,
        )
        logger.info(
            "å¯é€šè¿‡è®¾ç½®çŽ¯å¢ƒå˜é‡ PHOENIX_GRPC_PORT/PHOENIX_PORT æˆ– PHOENIX_ENABLED=false æ¥è§„é¿ã€‚"
        )
        return None
    except Exception as exc:
        logger.error(f"Phoenix å¯åŠ¨å¤±è´¥ï¼š{exc}")
        return None


def _get_llm_tokenizer():
    llm = getattr(Settings, "llm", None)
    if llm is None:
        return None
    for attr in ("tokenizer", "_tokenizer"):
        tok = getattr(llm, attr, None)
        if tok is not None:
            return tok
    return None


def _count_tokens(text: str, tokenizer=None) -> int:
    if not text:
        return 0
    if tokenizer is not None:
        try:
            return len(tokenizer.encode(text))
        except Exception as exc:
            logger.warning(f"token ç»Ÿè®¡å¤±è´¥ï¼Œä½¿ç”¨è¿‘ä¼¼å€¼: {exc}")
    # ç®€å•è¿‘ä¼¼ï¼šä¸­æ–‡æ–‡æœ¬å¹³å‡ 2 å­—ç¬¦ â‰ˆ 1 token
    return max(1, len(text) // 2)

async def main():
    # 1. åˆå§‹åŒ–æ¨¡åž‹
    ModelFactory.init_models()

    # å¯åŠ¨ Phoenix ç›‘æŽ§ï¼ˆå¦‚å¯ç”¨ï¼‰
    init_phoenix_monitor()

    # 2. åˆå§‹åŒ–å·¥å…·
    logger.info("åˆå§‹åŒ–å·¥å…·ç®±...")
    tool_cache: dict[str, dict] = {}
    last_focus = {"ts_code": None}

    def cache_tool_result(ts_code: str, category: str, summary: str, metadata=None):
        if not ts_code or not summary:
            return
        metadata = metadata or {}
        entry = tool_cache.setdefault(
            ts_code,
            {"ts_code": ts_code, "summaries": {}, "last_updated": None, "name": metadata.get("name")},
        )
        if metadata.get("name"):
            entry["name"] = metadata["name"]
        updated_at = datetime.now().strftime("%Y-%m-%d %H:%M")
        entry["summaries"][category] = {
            "summary": summary,
            "updated_at": updated_at,
        }
        combined_segments = []
        for cat, payload in entry["summaries"].items():
            combined_segments.append(
                f"{ts_code} | {cat} @ {payload['updated_at']}: {payload['summary']}"
            )
        entry["summary"] = "\n".join(combined_segments)
        entry["last_updated"] = updated_at
        last_focus["ts_code"] = ts_code

    def get_focus_hint() -> str:
        ts_code = last_focus.get("ts_code")
        if not ts_code:
            return ""
        entry = tool_cache.get(ts_code)
        if not entry:
            return ""
        display_name = entry.get("name") or ts_code
        summary = entry.get("summary")
        hint = f"æœ€è¿‘èšç„¦æ ‡çš„ï¼š{display_name}ï¼ˆ{ts_code}ï¼‰ã€‚"
        if summary:
            hint += f"\nç¼“å­˜è¦ç‚¹ï¼š\n{summary}"
        return hint

    def get_cache_snippets(limit: int = 3) -> list[str]:
        if not tool_cache:
            return []
        sorted_entries = sorted(
            tool_cache.values(),
            key=lambda item: item.get("last_updated") or "",
            reverse=True,
        )
        snippets = []
        for entry in sorted_entries[:limit]:
            snippets.append(entry.get("summary", ""))
        return [s for s in snippets if s]

    knowledge_base = FinancialKnowledgeBase()
    market_tool = MarketDataManager(cache_callback=cache_tool_result).get_tool()
    quant_tool = QuantAnalyzer(cache_callback=cache_tool_result).get_tool()

    all_tools = [tool for tool in [market_tool, quant_tool] if tool]

    # 3. æž„å»º Agent
    agent = ReActAgent(
        tools=all_tools,
        llm=Settings.llm,
        verbose=True,
        system_prompt=AGENT_SYSTEM_PROMPT,
        context=AGENT_CONTEXT_INJECTION,
        memory=ChatMemoryBuffer.from_defaults(token_limit=4096)
    )

    prompt_tokenizer = _get_llm_tokenizer()

    conversation_history = []

    def build_agent_input(user_query: str, rag_context: str) -> str:
        sections = []
        if conversation_history:
            recent_history = conversation_history[-3:]
            formatted_history = "\n".join(
                f"ðŸ‘¤ç”¨æˆ·: {turn['user']}\nðŸ¤–é¡¾é—®: {turn['assistant']}" for turn in recent_history
            )
            sections.append("ã€åŽ†å²å¯¹è¯ã€‘\n" + formatted_history)
        focus_hint = get_focus_hint()
        if focus_hint:
            sections.append("ã€ä¸Šä¸‹æ–‡æé†’ã€‘\n" + focus_hint)
        if rag_context:
            sections.append("ã€çŸ¥è¯†åº“æ£€ç´¢æ‘˜è¦ã€‘\n" + rag_context)
        sections.append("ã€å½“å‰ç”¨æˆ·é—®é¢˜ã€‘\n" + user_query)
        cache_snippets = get_cache_snippets()
        if cache_snippets:
            sections.append("ã€åŽ†å²å·¥å…·ç¼“å­˜ã€‘\n" + "\n".join(cache_snippets))
        sections.append(
            "è¯·ä¸¥æ ¼éµå¾ªï¼š\n"
            "1) å…ˆå°†ã€çŸ¥è¯†åº“æ£€ç´¢æ‘˜è¦ã€‘æ¦‚æ‹¬ä¸º 1-2 æ¡è¦ç‚¹ï¼Œæ”¾å…¥â€œã€ç†è®ºä¾æ®ã€‘â€ï¼Œå¥ä¸­æ ‡æ³¨â€œçŸ¥è¯†åº“â€ï¼›è‹¥åªæœ‰é»˜è®¤é£Žé™©å‡†åˆ™ï¼Œä¹Ÿå¿…é¡»å¼•ç”¨é»˜è®¤è¦ç‚¹å†ç»§ç»­ã€‚\n"
            "2) è‹¥éœ€è¦æ•°æ®ï¼Œè°ƒç”¨ market_data_tool / quant_analysis_toolï¼Œå¹¶åœ¨â€œã€æ•°æ®æ´žå¯Ÿã€‘â€éƒ¨åˆ†æ³¨æ˜Žæ¥æºä¸Žç»“è®ºã€‚\n"
            "3) åœ¨â€œã€é¡¾é—®å»ºè®®ã€‘â€éƒ¨åˆ†æ•´åˆç†è®º+æ•°æ®ï¼Œè¯´æ˜Žä»“ä½/æ­¢æŸ/é£Žé™©æç¤ºï¼Œå¦‚ä¿¡æ¯ä¸è¶³éœ€æ˜Žç¡®è¯´æ˜Žã€‚"
        )
        return "\n\n".join(sections)

    def build_fallback_response(user_query: str) -> str:
        focus_hint = get_focus_hint()
        cache_snippets = get_cache_snippets(limit=1)
        theory = "ã€ç†è®ºä¾æ®ã€‘çŸ¥è¯†åº“å“åº”å‡ºçŽ°å¼‚å¸¸ï¼Œä¸´æ—¶æ²¿ç”¨é»˜è®¤é£Žé™©æŽ§åˆ¶å‡†åˆ™ï¼ˆåˆ†æ•£é…ç½®ã€è®¾å®šæ­¢æŸã€æŽ§åˆ¶æ æ†ï¼‰ã€‚"
        if focus_hint:
            theory += f" è¿‘æœŸä¸Šä¸‹æ–‡ï¼š{focus_hint.replace(chr(10), ' ')}"
        if cache_snippets:
            data = "ã€æ•°æ®æ´žå¯Ÿã€‘æš‚æ— æ–°å¢žå·¥å…·ç»“æžœï¼Œæ²¿ç”¨ç¼“å­˜ï¼š\n" + cache_snippets[0]
        else:
            data = "ã€æ•°æ®æ´žå¯Ÿã€‘å½“å‰ç¼ºå°‘å¯å¤ç”¨çš„è¡Œæƒ…/é‡åŒ–æ•°æ®ï¼Œè¯·ç¨åŽé‡è¯•å·¥å…·æŸ¥è¯¢ã€‚"
        advice = (
            "ã€é¡¾é—®å»ºè®®ã€‘æš‚æ— æ³•å®Œæˆå®Œæ•´åˆ†æžï¼Œå»ºè®®å…ˆæ ¹æ®é»˜è®¤é£ŽæŽ§åŽŸåˆ™è¯„ä¼°é—®é¢˜ï¼š"
            f"â€œ{user_query}â€ã€‚è‹¥éœ€å³æ—¶ç»“æžœï¼Œè¯·é‡è¯•æˆ–ç¼©çŸ­æé—®ï¼Œå¹¶å¯æŒ‡å®šæ ‡çš„ä¸ŽæœŸæœ›ä»“ä½ã€‚"
        )
        return "\n".join([theory, data, advice])

    # 4. äº¤äº’å¾ªçŽ¯
    print("\nðŸ¤– é‡åŒ–æŠ•èµ„é¡¾é—®å·²å°±ç»ª (è¾“å…¥ 'exit' é€€å‡º)")
    while True:
        user_input = input("\nðŸ‘¤ ç”¨æˆ·: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        
        try:
            rag_context = knowledge_base.query_raw(user_input)

            enriched_input = build_agent_input(user_input, rag_context)
            token_count = _count_tokens(enriched_input, tokenizer=prompt_tokenizer)
            logger.info(
                f"[PromptStats] tokens={token_count} chars={len(enriched_input)} history_turns={len(conversation_history)}"
            )
            try:
                response = await asyncio.wait_for(agent.run(enriched_input), timeout=90)
            except asyncio.TimeoutError:
                logger.error("Agent å“åº”è¶…æ—¶ï¼Œè¿”å›žå…œåº•å»ºè®®ã€‚")
                response = build_fallback_response(user_input)
            except Exception as agent_exc:
                logger.error(f"Agent æ‰§è¡Œå¼‚å¸¸: {agent_exc}")
                response = build_fallback_response(user_input)
            conversation_history.append({"user": user_input, "assistant": str(response)})
            if len(conversation_history) > 5:
                conversation_history.pop(0)
            print(f"\nðŸ¤– é¡¾é—®: {response}")
        except Exception as e:
            logger.error(f"è¿è¡Œå‡ºé”™: {e}")
            fallback = build_fallback_response(user_input)
            conversation_history.append({"user": user_input, "assistant": fallback})
            if len(conversation_history) > 5:
                conversation_history.pop(0)
            print(f"\nðŸ¤– é¡¾é—®: {fallback}")

if __name__ == "__main__":
    asyncio.run(main())